{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2650d677",
   "metadata": {},
   "source": [
    "# Model Card: Daily Dairy Stock-out-Date Predictor  \n",
    "\n",
    "**Owner**: DS Team  \n",
    "**Version**: 1.0\n",
    "**Framework**: LightGBM 4.3  \n",
    "**Objective**: Predict the number of days until first out-of-stock (OOS) for each dairy SKU at store level.\n",
    "\n",
    "---\n",
    "\n",
    "## Data\n",
    "* **Source**: `features-daily_YYYY-MM_dairy.csv`\n",
    "* **Period**: 2024-11-01 to 2024-11-30\n",
    "* **Rows**: 54 740  \n",
    "* **Features used** (15): `DailyBOH`, `boh_mean_3`, `boh_std_3`, `boh_mean_7`, …, `DayofWeek`, `is_holiday`, `itemsku` (categorical)\n",
    "\n",
    "## Training & Evaluation\n",
    "* **Split**: time-based – 11/01-25 train, 11/26-30 hold-out  \n",
    "* **CV**: 5-fold `GroupKFold(itemsku)`\n",
    "* **Metrics**  \n",
    "  * CV MAE: **6.76 ± 0.29 days**  \n",
    "  * Hold-out MAE: **3.55 days**\n",
    "\n",
    "## Intended Use\n",
    "* Daily batch generation of **stock-out risk dates** for replenishment planning.\n",
    "* Not designed for intra-day (hourly) predictions.\n",
    "\n",
    "## Limitations\n",
    "* Single-month training – seasonality not yet captured.  \n",
    "* SKUs that never went OOS are handled by a rule (`days_to_oos = 0`); model does not learn their behavior.  \n",
    "* Holiday effects limited to US Thanksgiving; other holidays not encoded.\n",
    "\n",
    "## Ethical & Operational Considerations\n",
    "* False positives may trigger unnecessary replenishment; monitor MAE weekly.  \n",
    "* Retraining recommendation: monthly, or whenever data distribution shifts >10 %.\n",
    "\n",
    "## Changelog\n",
    "* **v11** – Handoff LightGBM model trained on 2024-11 dairy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7ccb7",
   "metadata": {},
   "source": [
    "# Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e86de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 54,740 | Cols: 39\n",
      "Unique itemsku: 1,247\n",
      "Items always zero in Nov: 212\n",
      "Permanent OOS list saved → models/prod_v1/permanent_oos_list.json  (212 SKUs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/km87dnjn7wvcyywzwry25sdh0000gn/T/ipykernel_35964/1677873271.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: first_zero_date(g.set_index(\"daydate\")[\"DailyBOH\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 4,299 | Test rows: 8,620\n",
      "Memory usage  (train): 5.04 MB\n"
     ]
    }
   ],
   "source": [
    "# stockout_preprocess.py    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 0. Parameters ----------\n",
    "DATA_PATH = Path(\"features-daily_2024-11_dairy.csv\")\n",
    "\n",
    "ROLL_WINDOWS = [3, 7]      # you can tweak later\n",
    "HOLIDAYS = pd.to_datetime([\"2024-11-28\", \"2024-11-29\", \"2024-11-30\"])\n",
    "\n",
    "# ---------- 1. Load ----------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"daydate\"] = pd.to_datetime(df[\"daydate\"])\n",
    "print(f\"Rows: {len(df):,} | Cols: {df.shape[1]}\")\n",
    "\n",
    "# ---------- 2. Basic EDA ----------\n",
    "n_items = df[\"itemsku\"].nunique()\n",
    "print(f\"Unique itemsku: {n_items:,}\")\n",
    "\n",
    "# identify items always zero\n",
    "always_zero = (\n",
    "    df.groupby(\"itemsku\")[\"DailyBOH\"]\n",
    "      .apply(lambda s: (s == 0).all())\n",
    "      .loc[lambda s: s]\n",
    "      .index.tolist()\n",
    ")\n",
    "print(f\"Items always zero in Nov: {len(always_zero):,}\")\n",
    "\n",
    "# ---------- 2b. Save permanent OOS list ----------\n",
    "import json\n",
    "PERM_OOS_PATH = Path(\"models/prod_v1/permanent_oos_list.json\")\n",
    "\n",
    "always_zero = (\n",
    "    df.groupby(\"itemsku\")[\"DailyBOH\"]\n",
    "      .apply(lambda s: (s == 0).all())\n",
    "      .loc[lambda s: s]\n",
    "      .index\n",
    "      .astype(str)            # convert to string for consistency\n",
    "      .tolist()\n",
    ")\n",
    "\n",
    "PERM_OOS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(PERM_OOS_PATH, \"w\") as f:\n",
    "    json.dump({\"generated_on\": str(pd.Timestamp.today().date()),\n",
    "               \"sku_count\": len(always_zero),\n",
    "               \"itemsku\":   always_zero},\n",
    "              f, indent=2)\n",
    "\n",
    "print(f\"Permanent OOS list saved → {PERM_OOS_PATH}  ({len(always_zero)} SKUs)\")\n",
    "\n",
    "# ---------- 3. Holiday flag ----------\n",
    "df[\"is_holiday\"] = df[\"daydate\"].isin(HOLIDAYS).astype(int)\n",
    "\n",
    "# ---------- 4. Create stock-out label ----------\n",
    "def first_zero_date(s):\n",
    "    zeros = s.index[s == 0]\n",
    "    return zeros.min() if len(zeros) else pd.NaT\n",
    "\n",
    "stockout_map = (\n",
    "    df.groupby(\"itemsku\")\n",
    "      .apply(lambda g: first_zero_date(g.set_index(\"daydate\")[\"DailyBOH\"]))\n",
    ")\n",
    "df = df.merge(\n",
    "    stockout_map.rename(\"oos_date\"),\n",
    "    on=\"itemsku\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# days_to_oos (positive integer; NaN if never stock-out within window)\n",
    "df[\"days_to_oos\"] = (df[\"oos_date\"] - df[\"daydate\"]).dt.days\n",
    "df.loc[df[\"days_to_oos\"] < 0, \"days_to_oos\"] = np.nan  # after OOS date → ignore\n",
    "\n",
    "# ---------- 5. Rolling features ----------\n",
    "df_sorted = df.sort_values([\"itemsku\", \"daydate\"])\n",
    "for win in ROLL_WINDOWS:\n",
    "    df_sorted[f\"boh_mean_{win}\"] = (\n",
    "        df_sorted.groupby(\"itemsku\")[\"DailyBOH\"]\n",
    "                 .transform(lambda x: x.rolling(win, min_periods=1).mean())\n",
    "    )\n",
    "    df_sorted[f\"boh_std_{win}\"] = (\n",
    "        df_sorted.groupby(\"itemsku\")[\"DailyBOH\"]\n",
    "                 .transform(lambda x: x.rolling(win, min_periods=1).std().fillna(0))\n",
    "    )\n",
    "\n",
    "# ---------- 6. Train / Test split ----------\n",
    "train_end = pd.Timestamp(\"2024-11-25\")\n",
    "train_mask = df_sorted[\"daydate\"] <= train_end\n",
    "test_mask  = df_sorted[\"daydate\"] > train_end\n",
    "\n",
    "train_df = df_sorted[train_mask & df_sorted[\"days_to_oos\"].notna() & ~df_sorted[\"itemsku\"].isin(always_zero)]\n",
    "test_df  = df_sorted[test_mask]\n",
    "\n",
    "print(f\"Train rows: {len(train_df):,} | Test rows: {len(test_df):,}\")\n",
    "print(f\"Memory usage  (train): {train_df.memory_usage(deep=True).sum()/1e6:.2f} MB\")\n",
    "\n",
    "# Optional: save processed parquet for faster reload\n",
    "train_df.to_parquet(\"train_proc.parquet\", index=False)\n",
    "test_df.to_parquet(\"test_proc.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ccf4d6",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "# [LightGBM] Model Building: Stockout Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b390fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2141\n",
      "[LightGBM] [Info] Number of data points in the train set: 3439, number of used features: 27\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2098\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 3439, number of used features: 27\n",
      "[LightGBM] [Info] Number of data points in the train set: 3440, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 7.842105\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2127\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 3439, number of used features: 27\n",
      "[LightGBM] [Info] Number of data points in the train set: 3439, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 7.862500\n",
      "[LightGBM] [Info] Start training from score 7.986333\n",
      "[LightGBM] [Info] Start training from score 8.054376\n",
      "[LightGBM] [Info] Start training from score 8.012794\n",
      "CV MAE (5-fold, grouped by itemsku): 6.38 ± 0.31\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2253\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 7.951617\n",
      "Hold-out MAE  : 3.68 days\n",
      "Hold-out MedAE: 3.27 days\n",
      "count    17.000000\n",
      "mean      3.202390\n",
      "std       2.143782\n",
      "min       0.441869\n",
      "50%       2.662693\n",
      "75%       4.031605\n",
      "90%       5.887064\n",
      "max       8.072347\n",
      "Name: item_mae, dtype: float64\n",
      "Model saved to /Users/alstonhsu/Desktop/stockoutPrediction/lgbm_stockout_model.txt\n",
      "Training parameters saved → models/prod_v1/params.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/km87dnjn7wvcyywzwry25sdh0000gn/T/ipykernel_35964/3180274867.py:94: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_eval.groupby(\"itemsku\")\n",
      "/var/folders/rc/km87dnjn7wvcyywzwry25sdh0000gn/T/ipykernel_35964/3180274867.py:95: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: np.mean(np.abs(g[\"days_to_oos\"] - g[\"pred\"])))\n"
     ]
    }
   ],
   "source": [
    "# train_lgbm_stockout.py    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "# ---------- 0. Parameters ----------\n",
    "TRAIN_PATH = Path(\"train_proc.parquet\")\n",
    "TEST_PATH  = Path(\"test_proc.parquet\")\n",
    "MODEL_OUT  = Path(\"lgbm_stockout_model.txt\")\n",
    "\n",
    "# ---------- 1. Load ----------\n",
    "train_df = pd.read_parquet(TRAIN_PATH)\n",
    "test_df  = pd.read_parquet(TEST_PATH)\n",
    "\n",
    "# permanent OOS list (saved during preprocessing)\n",
    "permanent_oos_list = (\n",
    "    train_df[\"itemsku\"].unique().tolist() +\n",
    "    test_df.loc[test_df[\"DailyBOH\"] == 0, \"itemsku\"].unique().tolist()\n",
    ")\n",
    "permanent_oos_list = list(set(permanent_oos_list))\n",
    "\n",
    "# ---------- 2. Feature selection ----------\n",
    "EXCLUDE = {\n",
    "    \"daydate\", \"oos_date\", \"days_to_oos\"\n",
    "}\n",
    "num_feats = [c for c in train_df.columns\n",
    "             if c not in EXCLUDE and\n",
    "                train_df[c].dtype != \"object\" and\n",
    "                not c.startswith(\"itemsku\")]\n",
    "\n",
    "CAT_FEATS = [\"itemsku\"]\n",
    "\n",
    "FEATURES = num_feats + CAT_FEATS\n",
    "\n",
    "# ensure categorical dtype\n",
    "for c in CAT_FEATS:\n",
    "    train_df[c] = train_df[c].astype(\"category\")\n",
    "    test_df[c]  = test_df[c].astype(\"category\")\n",
    "\n",
    "X = train_df[FEATURES]\n",
    "y = train_df[\"days_to_oos\"]\n",
    "\n",
    "# ---------- 3. Model ----------\n",
    "lgbm_params = dict(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    metric=\"mae\",\n",
    ")\n",
    "\n",
    "model = LGBMRegressor(**lgbm_params)\n",
    "\n",
    "# ---------- 4. Cross-validation ----------\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "cv_mae = -cross_val_score(\n",
    "    model, X, y,\n",
    "    cv=gkf.split(X, y, groups=train_df[\"itemsku\"]),\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "print(f\"CV MAE (5-fold, grouped by itemsku): {cv_mae.mean():.2f} ± {cv_mae.std():.2f}\")\n",
    "\n",
    "# ---------- 5. Fit on full train ----------\n",
    "model.fit(\n",
    "    X, y,\n",
    "    categorical_feature=CAT_FEATS,\n",
    ")\n",
    "\n",
    "# ---------- 6. Evaluate on hold-out (11/26–30) ----------\n",
    "test_eval = test_df[test_df[\"days_to_oos\"].notna()].copy()\n",
    "\n",
    "if len(test_eval) == 0:\n",
    "    print(\"⚠️  No stock-out events in 11/26–11/30 — skipping hold-out MAE.\")\n",
    "    overall_mae = overall_medae = None\n",
    "else:\n",
    "    test_eval[\"pred\"] = model.predict(test_eval[FEATURES])\n",
    "    overall_mae   = mean_absolute_error(test_eval[\"days_to_oos\"], test_eval[\"pred\"])\n",
    "    overall_medae = median_absolute_error(test_eval[\"days_to_oos\"], test_eval[\"pred\"])\n",
    "    print(f\"Hold-out MAE  : {overall_mae:.2f} days\")\n",
    "    print(f\"Hold-out MedAE: {overall_medae:.2f} days\")\n",
    "\n",
    "    # per-item MAE distribution\n",
    "    per_item = (\n",
    "        test_eval.groupby(\"itemsku\")\n",
    "                 .apply(lambda g: np.mean(np.abs(g[\"days_to_oos\"] - g[\"pred\"])))\n",
    "                 .rename(\"item_mae\")\n",
    "    )\n",
    "    print(per_item.describe(percentiles=[0.5, 0.75, 0.9]))\n",
    "\n",
    "# ---------- 6b. Feature Importance (after model.fit) ----------\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# imp_df = pd.DataFrame({\n",
    "#     \"feature\": model.feature_name_,\n",
    "#     \"gain\":    model.booster_.feature_importance(importance_type=\"gain\"),\n",
    "#     \"split\":   model.booster_.feature_importance(importance_type=\"split\")\n",
    "# }).sort_values(\"gain\", ascending=False)\n",
    "\n",
    "# print(imp_df.head(20))   # Top 20\n",
    "\n",
    "# ---------- 6c. Plotting ----------\n",
    "# topN = 15\n",
    "# imp_df.head(topN).plot.barh(\n",
    "#     x=\"feature\", y=\"gain\", figsize=(6, 5), legend=False\n",
    "# )\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title(\"Top Feature Gain\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# ---------- 7. Save model ----------\n",
    "model.booster_.save_model(str(MODEL_OUT))\n",
    "print(f\"Model saved to {MODEL_OUT.resolve()}\")\n",
    "\n",
    "# ---------- 8. Save params.yaml ----------\n",
    "def to_python(obj):\n",
    "    \"\"\"Recursively convert numpy scalars / arrays to builtin Python types.\"\"\"\n",
    "    if isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()                    # -> float or int\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_python(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [to_python(v) for v in obj]\n",
    "    return obj                               # str, bool, None, etc.\n",
    "\n",
    "PARAMS_PATH = Path(\"models/prod_v1/params.yaml\")\n",
    "PARAMS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "params_dict = {\n",
    "    \"generated_on\": str(pd.Timestamp.today()),\n",
    "    \"framework\":    \"LightGBM 4.3\",\n",
    "    \"hyperparams\":  lgbm_params,\n",
    "    \"categorical_features\": CAT_FEATS,\n",
    "    \"numeric_features\":     num_feats,\n",
    "    \"cv_mae_mean\":  float(cv_mae.mean()),\n",
    "    \"cv_mae_std\":   float(cv_mae.std()),\n",
    "    \"holdout_mae\":  None if overall_mae is None else float(overall_mae),\n",
    "    \"holdout_medae\": None if overall_medae is None else float(overall_medae),\n",
    "    \"training_rows\": int(len(train_df)),\n",
    "    \"training_period\": \"2024-11-01 → 2024-11-25\"\n",
    "}\n",
    "\n",
    "with open(PARAMS_PATH, \"w\") as f:\n",
    "    yaml.safe_dump(to_python(params_dict), f, sort_keys=False)\n",
    "\n",
    "print(f\"Training parameters saved → {PARAMS_PATH}\")\n",
    "\n",
    "# ---------- 8. Inference helper ----------\n",
    "def predict_days_to_oos(sample_df):\n",
    "    \"\"\"\n",
    "    Input  : DataFrame with same columns as `train_df`\n",
    "    Output : Series of predicted days_to_oos\n",
    "    Rule   : If itemsku in permanent_oos_list => 0\n",
    "    \"\"\"\n",
    "    sample_df = sample_df.copy()\n",
    "    sample_df[\"itemsku\"] = sample_df[\"itemsku\"].astype(\"category\")\n",
    "    pred = model.predict(sample_df[FEATURES])\n",
    "    pred = np.maximum(pred, 0)  # no negative days\n",
    "    pred = np.round(pred).astype(int)\n",
    "    pred[sample_df[\"itemsku\"].isin(permanent_oos_list)] = 0\n",
    "    return pd.Series(pred, index=sample_df.index, name=\"pred_days_to_oos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
